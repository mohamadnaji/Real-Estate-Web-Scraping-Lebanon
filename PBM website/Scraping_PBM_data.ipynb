{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18900,"status":"ok","timestamp":1719862070474,"user":{"displayName":"Mohamad Naji","userId":"14781419396110170563"},"user_tz":-180},"id":"gOIGj4xu6soo","outputId":"a65b7e9b-e207-42c2-9d14-f3803986b5f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.22.0)\n","Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n","Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.25.1)\n","Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.6.2)\n","Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n","Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n","Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"]}],"source":["pip install selenium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc3FwhcCUmvn"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import pandas as pd\n","import requests\n","from requests.exceptions import ConnectionError, Timeout, TooManyRedirects, RequestException\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.common.exceptions import TimeoutException\n","import time\n","import pandas as pd\n","import concurrent.futures"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5945,"status":"ok","timestamp":1719862078055,"user":{"displayName":"Mohamad Naji","userId":"14781419396110170563"},"user_tz":-180},"id":"m7YHHOE3bjDZ","outputId":"7a607938-12b2-4231-e6db-2065b0311b98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtX0ejEnFgIY"},"outputs":[],"source":["def scrape_page(url):\n","    data = {}\n","    try:\n","\n","        url = f'https://pbm-leb.com{url}'\n","\n","\n","        options = webdriver.ChromeOptions()\n","        options.add_argument('--headless')\n","        options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","        options.add_argument('--disable-extensions')\n","        options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n","\n","\n","        browser = webdriver.Chrome(options=options)\n","\n","        browser.get(url)\n","\n","        # Wait for the page to load\n","        # Adjust the selector and timeout as needed\n","        wait = WebDriverWait(browser, 10)\n","        element_present = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'leafletmap-template-1')))\n","\n","        # Scroll down incrementally\n","        # for i in range(10):  # Scroll down 10 times\n","        #     browser.execute_script(\"window.scrollBy(0, 800);\")\n","        #     time.sleep(0.2)  # Adjust sleep time if needed\n","\n","        # Get the rendered HTML\n","        html = browser.page_source\n","\n","        data['source'] = url\n","\n","        # Parse the HTML content\n","        soup = BeautifulSoup(html, 'html.parser')\n","\n","        # header that contains the price and location string\n","        header = soup.find('div', attrs={'data-id': '2'})\n","        locationStr = header.find('h2')\n","        price = header.find('p', attrs={'class': 'price'})\n","\n","        data['price'] = price.text.strip()\n","        data['location_str'] = locationStr.text.strip().split(\" in \")[1]\n","\n","        # body content details\n","        body = soup.find('div', attrs={'data-id': '4'})\n","\n","        body_details = body.find('div', attrs={'id': 'details'})\n","        body_details_list = body_details.find_all('p')\n","        for detail in body_details_list:\n","            value = detail.find('span').text.strip()\n","            detail.find('span').decompose()\n","            key = detail.text.strip()\n","            data[key.replace(':', '')] = value\n","\n","        # body content area data\n","        data['area'] = ''\n","        h3 = body.find('h3')\n","        ul = h3.find_next_sibling()\n","        lis = ul.find_all('li')\n","        for li in lis:\n","            value = li.text.strip()\n","            value = value.replace('- ', '')\n","            value = ' '.join(value.split())\n","            data['area'] += value + '; '\n","\n","        # map coordinate url\n","        map = soup.find('div', {'class' : 'leafletmap-template-1'})\n","        img = map.find('img')\n","        data['map'] = img.get('src')\n","\n","        # Proximities\n","        proximities = soup.find('div', {'class' : 'template-8'})\n","        data['proximities'] = proximities.text.strip()\n","\n","        # Services\n","        service = body.find('div', {'class' : 'property-info-template-6'})\n","        services = service.find_all('li')\n","        data['services'] = ''\n","        for li in services:\n","            value = li.text.strip()\n","            data['services'] += value + '; '\n","\n","\n","        # info\n","        info = body.find('div', {'class' : 'info'})\n","        parag = info.find('p')\n","        data['info'] = parag.text.strip()\n","\n","        # description\n","        description = soup.find('p', {'id' : 'description'})\n","        data['description'] = description.text.strip()\n","        print(data)\n","        return data\n","    finally:\n","        browser.quit()  # Close the browser even if an exception occurs\n","\n","    # except TimeoutException:\n","    #     print(\"Timed out waiting for page to load\")\n","    # except ConnectionError as e:\n","    #     print(f\"Connection error occurred: {e}\")\n","    # except Timeout as e:\n","    #     print(f\"Timeout error occurred: {e}\")\n","    # except TooManyRedirects as e:\n","    #     print(f\"Too many redirects: {e}\")\n","    # except RequestException as e:\n","    #     print(f\"An error occurred RequestException: {e}\")\n","    # except Exception as e:\n","    #     print(f\"An error occurred: {e}\")\n","\n","    # return None"]},{"cell_type":"code","source":["def scraping_data(path, file_name):\n","    # Load the CSV file into a DataFrame\n","    urls = pd.read_csv(path + file_name)\n","    print(len(urls))\n","    all_data = []\n","    failed_url = []\n","    index = 2\n","    def process_url(url):\n","        print(f\"Processing {url}\")\n","        return scrape_page(url)\n","\n","    with concurrent.futures.ThreadPoolExecutor() as executor:\n","        # Map the process_url function to the list of URLs\n","        future_to_url = {executor.submit(process_url, url['URL']): url for index, url in urls.iterrows()}\n","\n","        for future in concurrent.futures.as_completed(future_to_url):\n","            url = future_to_url[future]\n","            try:\n","                page_data = future.result()\n","                all_data.append(page_data)\n","                index += 1\n","                print(index)\n","            except Exception as exc:\n","                print(f\"{url['URL']} generated an exception: {exc}\")\n","                failed_url.append(url['URL'])\n","\n","    # Filter out None values\n","    filtered_data = [item for item in all_data if item is not None]\n","\n","    # Create DataFrame\n","    df = pd.DataFrame(filtered_data)\n","\n","    csv_file_path = f'{path}data_{file_name}'\n","    df.to_csv(csv_file_path, index=False)\n","    return all_data, failed_url"],"metadata":{"id":"wxvxy5JoP11y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1o1krsSbjPVU1H8_JznVhasN9JmlDvrzB"},"id":"UjwJKA6t1mFj","outputId":"a0e7e7d2-7010-4bc0-fee2-4bee1cf4ea1a","executionInfo":{"status":"ok","timestamp":1719872157285,"user_tz":-180,"elapsed":9681030,"user":{"displayName":"Mohamad Naji","userId":"14781419396110170563"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Replace the path with your file's path\n","# file_path_rent = '/content/drive/MyDrive/M2 final project/PBM website/'\n","file_path_sale = '/content/drive/MyDrive/M2 final project/PBM website/'\n","d,f = scraping_data(file_path_sale, 'urls_to_be_scraped_sale.csv')\n","# scraping_data(file_path_rent, 'urls_to_be_scraped_rent.csv')"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuFQsmBqXLgNdXKJfUKwha"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}